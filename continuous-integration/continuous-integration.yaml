---
AWSTemplateFormatVersion: '2010-09-09'
Description: Continuous integration stack for serverless web chat project.

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: Options
        Parameters:
          - DeployS3ObjectsByFilename
      - Label:
          default: Deployment
        Parameters:
          - SourceLocationType
          - SourceLocation
          - SourceVersion
    ParameterLabels:
      DeployS3ObjectsByFilename:
        default: Deploy S3 Objects By File Name
      DockerImageBuildVersion:
        default: Docker Image Build Version
      SourceLocation:
        default: Source Location
      SourceLocationType:
        default: Source Location Type
      SourceVersion:
        default: Source Version

Parameters:
  DeployS3ObjectsByFilename:
    Type: String
    Default: 'No'
    Description: |+
      Not necessary unless you intend to make your Lambda packages available 
      for others to launch the main stack easily.
    AllowedValues:
      - 'Yes'
      - 'No'
  DockerImageBuildVersion:
    Type: String
    Description: Change this value on stack update to rebuild the Docker image used for building.
    Default: ''
  SourceLocationType:
    Type: String
    Default: GITHUB
    AllowedValues:
      - GITHUB
  SourceLocation:
    Type: String
    Default: https://github.com/moduspwnens/boa-chat.git
  SourceVersion:
    Type: String
    Description: Commit ID, branch name, or tag name from the GitHub repository.
    Default: 0.1

Conditions:
  DeployS3ObjectsByFilenameCondition:
    Fn::Equals:
      - Ref: DeployS3ObjectsByFilename
      - 'Yes'
  UseHeadOfLineSourceVersionCondition:
    Fn::Equals:
      - Ref: SourceVersion
      - ''

Resources:
  
  MainRepository:
    Type: AWS::CodeCommit::Repository
    Properties:
      RepositoryName:
        Fn::Sub: ${ProjectGlobalPrefix.Prefix}-webchat
  
  ArtifactStoreBucket:
    Type: AWS::S3::Bucket
  
  
  
  DockerBuilderImageRepository:
    Type: AWS::ECR::Repository
    Properties:
      RepositoryName:
        Fn::Sub: ${ProjectGlobalPrefix.Prefix}-webchat-docker-builder
        
      # Note that the account IDs below are internal CodeCommit account IDs.
      # https://docs.aws.amazon.com/codebuild/latest/userguide/sample-ecr.html
      RepositoryPolicyText:
        Fn::Sub: |-
          {
            "Version": "2008-10-17",
            "Statement": [
              {
                "Sid": "CodeBuildAccess",
                "Effect": "Allow",
                "Principal": {
                  "AWS": [
                    "arn:aws:iam::201349592320:root",
                    "arn:aws:iam::570169269855:root",
                    "arn:aws:iam::964771811575:root"
                  ]
                },
                "Action": [
                  "ecr:GetDownloadUrlForLayer",
                  "ecr:BatchGetImage",
                  "ecr:BatchCheckLayerAvailability"
                ]
              }
            ]
          }
  
  InitialSourceRepositoryPusherCodeBuildRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - codebuild.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: RoleActions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource:
                  - Fn::Sub: ${InitialSourceRepositoryPusherCodeBuildLogGroup.Arn}
              - Effect: Allow
                Action:
                  - codecommit:GitPush
                Resource:
                  - Fn::Sub: arn:aws:codecommit:${AWS::Region}:${AWS::AccountId}:${ProjectGlobalPrefix.Prefix}-webchat
        - PolicyName: BennActions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - codecommit:GitPull
                Resource:
                  - arn:aws:codecommit:us-east-1:105745236650:aws-serverless-web-chat
  
  InitialSourceRepositoryPusherCodeBuildLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName:
        Fn::Sub: /aws/codebuild/${ProjectGlobalPrefix.Prefix}-webchat-source-push
  
  InitialSourceRepositoryPusherCodeBuildProject:
    Type: AWS::CodeBuild::Project
    Properties:
      Artifacts:
        Type: NO_ARTIFACTS
      Environment:
        ComputeType: BUILD_GENERAL1_SMALL
        Image: aws/codebuild/docker:1.12.1
        Type: LINUX_CONTAINER
        EnvironmentVariables:
          - Name: AWS_DEFAULT_REGION
            Value:
              Ref: AWS::Region
          - Name: CODECOMMIT_SOURCE_URL
            Value:
              Ref: SourceLocation
          - Name: CODECOMMIT_TARGET_URL
            Value:
              Fn::GetAtt:
                - MainRepository
                - CloneUrlHttp
          - Name: CODECOMMIT_SOURCE_VERSION
            Value:
              Fn::If:
                - UseHeadOfLineSourceVersionCondition
                - HEAD
                - Ref: SourceVersion
      Name:
        Fn::Sub: ${ProjectGlobalPrefix.Prefix}-webchat-source-push
      Description: Performs the initial push into the new CodeCommit repository.
      ServiceRole:
        Fn::Sub: ${InitialSourceRepositoryPusherCodeBuildRole.Arn}
      Source:
        Location:
          Ref: SourceLocation
        Type:
          Ref: SourceLocationType
        BuildSpec: |-
          version: 0.1
          
          phases:
            pre_build:
              commands:
                - git config --global push.default simple
                - git config --global credential.helper '!aws codecommit credential-helper $@'
                - git config --global credential.UseHttpPath true
                - git config --global push.followTags true
                - rm -rf *
                - rm -rf .git
            build:
              commands:
                - echo Build started on `date`
                - git clone "$CODECOMMIT_SOURCE_URL" src; mv src/* src/.git* .; rmdir src
                - git checkout -b newbranch "$CODECOMMIT_SOURCE_VERSION"
                - git remote add codecommit "$CODECOMMIT_TARGET_URL"
                - git push codecommit newbranch:master
            post_build:
              commands:
                - echo Build completed on `date`
      TimeoutInMinutes: 15
  
  InitialSourceRepositoryPush:
    Type: Custom::CodeBuildInstance
    Properties:
      ServiceToken:
        Fn::Sub: ${CodeBuildInstanceFunction.Arn}
      CodeBuildProject:
        Ref: InitialSourceRepositoryPusherCodeBuildProject
      StateMachineArn:
        Fn::Sub: ${CodeBuildInstanceStateMachine.StateMachineArn}
  
  
  DockerBuilderCodeBuildRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - codebuild.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: RoleActions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource:
                  - Fn::Sub: ${DockerBuilderCodeBuildLogGroup.Arn}
              - Effect: Allow
                Action:
                  - codecommit:GitPull
                Resource:
                  - Fn::Sub: arn:aws:codecommit:${AWS::Region}:${AWS::AccountId}:${ProjectGlobalPrefix.Prefix}-webchat
              - Effect: Allow
                Action:
                  - ecr:GetAuthorizationToken
                Resource: '*'
              - Effect: Allow
                Action:
                  - ecr:BatchCheckLayerAvailability
                  - ecr:BatchGetImage
                  - ecr:GetDownloadUrlForLayer
                Resource:
                  Fn::Sub: arn:aws:ecr:${AWS::Region}:137112412989:repository/amazonlinux
              - Effect: Allow
                Action:
                  - ecr:BatchCheckLayerAvailability
                  - ecr:CompleteLayerUpload
                  - ecr:InitiateLayerUpload
                  - ecr:PutImage
                  - ecr:UploadLayerPart
                Resource:
                  Fn::Sub: arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/${ProjectGlobalPrefix.Prefix}-webchat-docker-builder
  
  DockerBuilderCodeBuildLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName:
        Fn::Sub: /aws/codebuild/${ProjectGlobalPrefix.Prefix}-webchat-docker-builder
  
  DockerBuilderCodeBuildProject:
    Type: AWS::CodeBuild::Project
    Properties:
      Artifacts:
        Type: NO_ARTIFACTS
      Environment:
        ComputeType: BUILD_GENERAL1_SMALL
        Image: aws/codebuild/docker:1.12.1
        Type: LINUX_CONTAINER
        EnvironmentVariables:
          - Name: AWS_DEFAULT_REGION
            Value:
              Ref: AWS::Region
          - Name: AWS_ACCOUNT_ID
            Value:
              Ref: AWS::AccountId
          - Name: IMAGE_REPO_NAME
            Value:
              Ref: DockerBuilderImageRepository
          - Name: IMAGE_TAG
            Value: latest
      Name:
        Fn::Sub: ${ProjectGlobalPrefix.Prefix}-webchat-docker-builder
      Description: Builds the Docker image used for normal Lambda package builds.
      ServiceRole:
        Fn::Sub: ${DockerBuilderCodeBuildRole.Arn}
      Source:
        Location:
          Fn::Sub: ${MainRepository.CloneUrlHttp}
        Type: CODECOMMIT
        BuildSpec: |-
          version: 0.1
          
          phases:
            pre_build:
              commands:
                - echo Logging in to Amazon ECR...
                - $(aws ecr get-login --region $AWS_DEFAULT_REGION --registry-ids 137112412989)
                - docker pull 137112412989.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/amazonlinux:latest
                - docker tag 137112412989.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/amazonlinux:latest amazonlinux:latest
                - $(aws ecr get-login --region $AWS_DEFAULT_REGION)
            build:
              commands:
                - echo Build started on `date`
                - echo Building the Docker image...
                - docker build -f continuous-integration/Dockerfile -t $IMAGE_REPO_NAME .
                - docker tag $IMAGE_REPO_NAME:$IMAGE_TAG $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG
            post_build:
              commands:
                - echo Build completed on `date`
                - echo Pushing the Docker image...
                - docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/$IMAGE_REPO_NAME:$IMAGE_TAG
      TimeoutInMinutes: 45
  
  InitialDockerImageBuild:
    Type: Custom::CodeBuildInstance
    Properties:
      ServiceToken:
        Fn::Sub: ${CodeBuildInstanceFunction.Arn}
      CodeBuildProject:
        Ref: DockerBuilderCodeBuildProject
      StateMachineArn:
        Fn::Sub: ${CodeBuildInstanceStateMachine.StateMachineArn}
      Version:
        Ref: DockerImageBuildVersion
    DependsOn:
      - InitialSourceRepositoryPush
  
  MainCodeBuildProject:
    Type: AWS::CodeBuild::Project
    Properties:
      Artifacts:
        Type: CODEPIPELINE
      Environment:
        ComputeType: BUILD_GENERAL1_SMALL
        Image:
          Fn::Sub: ${AWS::AccountId}.dkr.ecr.${AWS::Region}.amazonaws.com/${ProjectGlobalPrefix.Prefix}-webchat-docker-builder:latest
        Type: LINUX_CONTAINER
      Name:
        Fn::Sub: ${ProjectGlobalPrefix.Prefix}-webchat
      ServiceRole:
        Fn::Sub: ${MainCodeBuildRole.Arn}
      Source:
        Location: CODEPIPELINE
        Type: CODEPIPELINE
      TimeoutInMinutes: 30
  
  MainCodeBuildRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - codebuild.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: CodeBuildRoleActions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource:
                  - Fn::Sub: arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:/aws/codebuild/${ProjectGlobalPrefix.Prefix}-webchat:*
              - Effect: Allow
                Action:
                  - codecommit:GitPull
                Resource:
                  - Fn::Sub: arn:aws:codecommit:${AWS::Region}:${AWS::AccountId}:${ProjectGlobalPrefix.Prefix}-webchat
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                Resource:
                  Fn::Sub: arn:aws:s3:::${ArtifactStoreBucket}/*
              - Effect: Allow
                Action:
                  - s3:ListBucket
                Resource:
                  Fn::Sub: arn:aws:s3:::${ArtifactStoreBucket}
  
  MainCodeBuildLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName:
        Fn::Sub: /aws/codebuild/${ProjectGlobalPrefix.Prefix}-webchat
  
  BuildPipeline:
    Type: AWS::CodePipeline::Pipeline
    Properties:
      ArtifactStore:
        Location:
          Ref: ArtifactStoreBucket
        Type: S3
      DisableInboundStageTransitions:
        - Fn::If:
            - DeployS3ObjectsByFilenameCondition
            - Ref: AWS::NoValue
            - StageName: DeployS3ArtifactsByFilenames
              Reason: Disabled per continuous integration stack parameter
      RoleArn:
        Fn::Sub: ${BuildPipelineRole.Arn}
      Stages:
        - Name: Source
          Actions:
            - Name: CodeCommitSource
              ActionTypeId:
                Category: Source
                Owner: AWS
                Provider: CodeCommit
                Version: 1
              Configuration:
                BranchName: master
                RepositoryName:
                  Fn::Sub: ${ProjectGlobalPrefix.Prefix}-webchat
              OutputArtifacts:
                - Name: AppSource
        - Name: Build
          Actions:
            - Name: Build
              ActionTypeId:
                Category: Build
                Owner: AWS
                Provider: CodeBuild
                Version: 1
              InputArtifacts:
                - Name: AppSource
              Configuration:
                ProjectName:
                  Ref: MainCodeBuildProject
              OutputArtifacts:
                - Name: AppPackage
            - Name: GetStackTemplate
              ActionTypeId:
                Category: Invoke
                Owner: AWS
                Provider: Lambda
                Version: 1
              InputArtifacts:
                - Name: AppSource
              Configuration:
                FunctionName:
                  Ref: CodePipelineArtifactExtractorFunction
                UserParameters: |-
                  {
                    "FilePath": "serverless-web-chat-api.yaml"
                  }
              OutputArtifacts:
                - Name: SourceCloudFormationStackTemplate
        - Name: Deploy
          Actions:
            - Name: DeployS3ArtifactsByHashes
              ActionTypeId:
                Category: Invoke
                Owner: AWS
                Provider: Lambda
                Version: 1
              InputArtifacts:
                - Name: AppPackage
              Configuration:
                FunctionName:
                  Ref: S3UnzipUploaderFunction
                UserParameters:
                  Fn::Sub: |-
                    {
                      "Bucket": "${ArtifactStoreBucket}",
                      "HashKeys": true
                    }
              OutputArtifacts:
                - Name: DeployedArtifactMap
              RunOrder: 1
            - Name: UpdateTemplateResourceS3Keys
              ActionTypeId:
                Category: Invoke
                Owner: AWS
                Provider: Lambda
                Version: 1
              InputArtifacts:
                - Name: SourceCloudFormationStackTemplate
                - Name: DeployedArtifactMap
              Configuration:
                FunctionName:
                  Ref: CloudFormationTemplateS3KeyUpdaterFunction
                UserParameters:
                  Fn::Sub: |-
                    {
                      
                    }
              OutputArtifacts:
                - Name: ResultCloudFormationStackTemplate
              RunOrder: 2
            - Name: DeployStack
              ActionTypeId:
                Category: Invoke
                Owner: AWS
                Provider: Lambda
                Version: 1
              InputArtifacts:
                - Name: ResultCloudFormationStackTemplate
              Configuration:
                FunctionName:
                  Ref: CloudFormationS3StackDeployerFunction
                UserParameters:
                  Fn::Sub: |-
                    {
                      "Capabilities": "CAPABILITY_IAM",
                      "RoleArn": "${StackDeployCloudFormationRole.Arn}",
                      "StackName": "${ProjectGlobalPrefix.Prefix}-webchat",
                      "ParameterOverrides": {
                          "S3SourceType": "S3 Bucket",
                          "S3SourceName": "${ArtifactStoreBucket}"
                      }
                    }
              OutputArtifacts:
                - Name: CloudFormationStack
              RunOrder: 3
        - Name: DeployS3ArtifactsByFilenames
          Actions:
            - Name: DeployS3ArtifactsByFilenames
              ActionTypeId:
                Category: Invoke
                Owner: AWS
                Provider: Lambda
                Version: 1
              InputArtifacts:
                - Name: AppPackage
              Configuration:
                FunctionName:
                  Ref: S3UnzipUploaderFunction
                UserParameters:
                  Fn::Sub: |-
                    {
                      "Bucket": "${ArtifactStoreBucket}",
                      "HashKeys": false
                    }
              OutputArtifacts:
                - Name: DeployedArtifactsByFilenameMap
              RunOrder: 1
    DependsOn:
      - InitialDockerImageBuild
  
  BuildPipelineRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - codepipeline.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: RoleActions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - codecommit:CancelUploadArchive
                  - codecommit:GetBranch
                  - codecommit:GetCommit
                  - codecommit:GetUploadArchiveStatus
                  - codecommit:UploadArchive
                Resource:
                  - Fn::Sub: arn:aws:codecommit:${AWS::Region}:${AWS::AccountId}:${ProjectGlobalPrefix.Prefix}-webchat
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:PutObject
                Resource:
                  Fn::Sub: arn:aws:s3:::${ArtifactStoreBucket}/*
              - Effect: Allow
                Action:
                  - s3:GetBucketVersioning
                Resource:
                  Fn::Sub: arn:aws:s3:::${ArtifactStoreBucket}
              - Effect: Allow
                Action:
                  - codebuild:StartBuild
                  - codebuild:BatchGetBuilds
                Resource:
                  Fn::Sub: ${MainCodeBuildProject.Arn}
              - Effect: Allow
                Action:
                  - lambda:ListFunctions
                Resource: '*'
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource:
                  - Fn::Sub: ${CodePipelineArtifactExtractorFunction.Arn}
                  - Fn::Sub: ${CloudFormationS3StackDeployerFunction.Arn}
                  - Fn::Sub: ${S3UnzipUploaderFunction.Arn}
                  - Fn::Sub: ${CloudFormationTemplateS3KeyUpdaterFunction.Arn}
                
  StackDeployCloudFormationRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - cloudformation.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: RoleActions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                Resource:
                  Fn::Sub: arn:aws:s3:::${ArtifactStoreBucket}/*
        - PolicyName: MainStackActions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - apigateway:PATCH
                  - apigateway:POST
                  - iam:AttachRolePolicy
                  - iam:CreateRole
                  - iam:DetachRolePolicy
                  - iam:PutRolePolicy
                  - lambda:AddPermission
                  - lambda:CreateFunction
                  - lambda:UpdateFunctionCode
                  - lambda:UpdateFunctionConfiguration
                  - logs:CreateLogGroup
                  - logs:PutRetentionPolicy
                  - s3:CreateBucket
                  - s3:PutBucketPolicy
                  - s3:PutBucketWebsite
                  - s3:PutLifecycleConfiguration
                Resource: '*'
              - Effect: Allow
                Action:
                  - apigateway:GET
                  - iam:GetRole
                  - iam:PassRole
                  - lambda:GetFunction
                  - lambda:GetFunctionConfiguration
                  - lambda:InvokeFunction
                  - logs:DescribeLogGroups
                  - s3:GetObject
                Resource: '*'
              - Effect: Allow
                Action:
                  - apigateway:DELETE
                  - iam:DeleteRole
                  - iam:DeleteRolePolicy
                  - lambda:DeleteFunction
                  - lambda:RemovePermission
                  - logs:DeleteLogGroup
                  - s3:DeleteBucket
                  - s3:DeleteBucketPolicy
                Resource: '*'
  
  
  
  
  CloudFormationS3StackDeployerFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName:
        Fn::Sub: ${ProjectGlobalPrefix.Prefix}-CloudFormationS3StackDeployerFunction
      Description: Creates or updates a CloudFormation stack using a template on S3
      Handler: index.lambda_handler
      MemorySize: 128
      Role:
        Fn::Sub: ${CloudFormationS3StackDeployerFunctionRole.Arn}
      Code:
        ZipFile: |-
          import os
          import json
          import boto3
          import botocore

          cloudformation_client = boto3.client("cloudformation")
          codepipeline_client = boto3.client("codepipeline")

          def lambda_handler(event, context):
              print("Event: {}".format(json.dumps(event)))
    
              job_props = event["CodePipeline.job"]
              artifact_creds = job_props["data"]["artifactCredentials"]
              user_params = json.loads(job_props["data"]["actionConfiguration"]["configuration"]["UserParameters"])
    
              stack_name = user_params["StackName"]
              stack_parameters = user_params.get("ParameterOverrides", {})
              
              if isinstance(stack_parameters, basestring):
                  stack_parameters = json.loads(stack_parameters)
    
              stack_already_exists = False
              create_update_ok = False
    
              try:
                  response = cloudformation_client.describe_stacks(
                      StackName = stack_name
                  )
                  
                  this_stack = response["Stacks"][0]
                  
                  if this_stack["StackStatus"] == "ROLLBACK_COMPLETE":
                      cloudformation_client.delete_stack(
                          StackName = this_stack["StackId"]
                      )
    
                      stack_already_exists = True
                      create_update_ok = False
                  
                  elif this_stack["StackStatus"].endswith("_COMPLETE"):
                      stack_already_exists = True
                      create_update_ok = True
                      
                  print(this_stack)
        
              except botocore.exceptions.ClientError as e:
                  if e.response['Error']['Code'] == 'ValidationError':
                      # Stack doesn't exist yet.
                      create_update_ok = True
                  else:
                      raise
    
              input_artifact_s3_location = job_props["data"]["inputArtifacts"][0]["location"]["s3Location"]
              s3_template_url = "https://s3.amazonaws.com/{}/{}".format(
                  input_artifact_s3_location["bucketName"],
                  input_artifact_s3_location["objectKey"]
              )
              cloudformation_role_arn = user_params["RoleArn"]
              capabilities_list = user_params["Capabilities"].split(",")
              
              print("S3 template URL: {}".format(s3_template_url))
              
              stack_id = None
              
              parameter_list = []
              
              for each_key in stack_parameters.keys():
                  parameter_list.append({
                      "ParameterKey": each_key,
                      "ParameterValue": stack_parameters[each_key]
                  })
              
              if create_update_ok and not stack_already_exists:
                  response = cloudformation_client.create_stack(
                      StackName = stack_name,
                      TemplateURL = s3_template_url,
                      RoleARN = cloudformation_role_arn,
                      Capabilities = capabilities_list,
                      Parameters = parameter_list
                  )
        
                  stack_id = response["StackId"]
              
              elif create_update_ok:
                  
                  try:
                      response = cloudformation_client.update_stack(
                          StackName = stack_name,
                          TemplateURL = s3_template_url,
                          RoleARN = cloudformation_role_arn,
                          Capabilities = capabilities_list,
                          Parameters = parameter_list
                      )
                      
                      stack_id = response["StackId"]
                      
                  except botocore.exceptions.ClientError as e:
                      if e.response['Error']['Code'] == 'ValidationError' and 'No updates are to be performed.' in str(e):
                          stack_id = "NoUpdatePerformed"
                      else:
                          raise
              
              else:
                  
                  codepipeline_client.put_job_failure_result(
                      jobId = job_props["id"],
                      failureDetails = {
                          "type": "JobFailed",
                          "message": "Stack not in a valid state for a create / update."
                      }
                  )
                  
                  return {}
                  
              
              print("Stack ID: {}".format(stack_id))
    
              codepipeline_client.put_job_success_result(
                  jobId = job_props["id"]
              )
    
              return {}
              
      Runtime: python2.7
      Timeout: 300

  CloudFormationS3StackDeployerFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: RoleActions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource:
                  - Fn::Sub: ${CloudFormationS3StackDeployerFunctionLogGroup.Arn}
              - Effect: Allow
                Action:
                  - codepipeline:PutJobSuccessResult
                  - codepipeline:PutJobFailureResult
                Resource: '*'
              - Effect: Allow
                Action:
                  - s3:GetObject
                Resource:
                  Fn::Sub: arn:aws:s3:::${ArtifactStoreBucket}/*
              - Effect: Allow
                Action:
                  - cloudformation:CreateStack
                  - cloudformation:DeleteStack
                  - cloudformation:DescribeStacks
                  - cloudformation:UpdateStack
                Resource:
                  Fn::Sub: arn:aws:cloudformation:${AWS::Region}:${AWS::AccountId}:stack/${ProjectGlobalPrefix.Prefix}-webchat/*
              - Effect: Allow
                Action:
                  - iam:PassRole
                Resource:
                  Fn::Sub: ${StackDeployCloudFormationRole.Arn}
  
  CloudFormationS3StackDeployerFunctionLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName:
        Fn::Sub: /aws/lambda/${ProjectGlobalPrefix.Prefix}-CloudFormationS3StackDeployerFunction
  
  
  
  S3UnzipUploaderFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName:
        Fn::Sub: ${ProjectGlobalPrefix.Prefix}-S3UnzipUploaderFunction
      Description: Downloads a ZIP file from S3, decompresses it, and uploads its contents to S3
      Handler: index.lambda_handler
      MemorySize: 1536
      Role:
        Fn::Sub: ${S3UnzipUploaderFunctionRole.Arn}
      Code:
        ZipFile: |-
          import os, json, hashlib, tempfile, zipfile, shutil
          import boto3, botocore
          from botocore.client import Config
          
          codepipeline_client = boto3.client("codepipeline")
          s3_client = boto3.client("s3", config=Config(signature_version='s3v4'))
          
          def lambda_handler(event, context):
              print("Event: {}".format(json.dumps(event)))
              
              job_props = event["CodePipeline.job"]
              artifact_creds = job_props["data"]["artifactCredentials"]
              user_params = json.loads(job_props["data"]["actionConfiguration"]["configuration"]["UserParameters"])
              
              artifact_session = boto3.session.Session(
                  aws_access_key_id = artifact_creds["accessKeyId"],
                  aws_secret_access_key = artifact_creds["secretAccessKey"],
                  aws_session_token = artifact_creds["sessionToken"]
              )
              
              artifact_s3_client = artifact_session.client("s3", config=Config(signature_version='s3v4'))
              
              f = tempfile.NamedTemporaryFile(delete=False)
              f.close()
              input_temp_file_path = f.name
              f = tempfile.NamedTemporaryFile(delete=False)
              f.close()
              temp_dir_path = f.name
              os.unlink(temp_dir_path)
              os.makedirs(temp_dir_path)
              
              input_artifact_s3_location = job_props["data"]["inputArtifacts"][0]["location"]["s3Location"]
              input_artifact_bucket_name = input_artifact_s3_location["bucketName"]
              input_artifact_object_key = input_artifact_s3_location["objectKey"]
              
              print("Downloading input artifact from {} {}".format(
                  input_artifact_bucket_name,
                  input_artifact_object_key
              ))
              
              artifact_s3_client.download_file(
                  input_artifact_bucket_name,
                  input_artifact_object_key,
                  input_temp_file_path
              )
              
              input_zfile = zipfile.ZipFile(input_temp_file_path)
              input_zfile.extractall(temp_dir_path)
              input_zfile.close()
              
              output_artifact_map = {}
              
              target_bucket_name = user_params["Bucket"]
              hash_object_prefix = "deploy-md5/"
              
              for root, dir_list, file_list in os.walk(temp_dir_path):
                  for each_file in file_list:
                      each_file_abs_path = os.path.join(root, each_file)
                      each_file_relative_path = each_file_abs_path[len(temp_dir_path)+1:]
                      md5_hash = file_md5(each_file_abs_path)
          
                      target_object_key = each_file_relative_path
          
                      if user_params.get("HashKeys"):
                          target_object_key = hash_object_prefix + md5_hash
          
                      output_artifact_map[each_file_relative_path] = target_object_key
          
                      print("Uploading {}.".format(each_file_relative_path))
            
                      s3_client.upload_file(
                          each_file_abs_path,
                          target_bucket_name,
                          target_object_key,
                          ExtraArgs = {
                              "Metadata": {
                                  "sha256base64": file_sha256_checksum_base64(each_file_abs_path),
                                  "filename": each_file_relative_path,
                                  "md5": md5_hash
                              }
                          }
                      )
              
              output_artifact_s3_location = job_props["data"]["outputArtifacts"][0]["location"]["s3Location"]
              
              artifact_s3_client.put_object(
                  Bucket = output_artifact_s3_location["bucketName"],
                  Key = output_artifact_s3_location["objectKey"],
                  Body = json.dumps(output_artifact_map)
              )
              
              if os.path.exists(input_temp_file_path):
                  os.unlink(input_temp_file_path)
              
              if os.path.exists(temp_dir_path):
                  shutil.rmtree(temp_dir_path)
              
              codepipeline_client.put_job_success_result(
                  jobId = job_props["id"]
              )
          
          #http://stackoverflow.com/a/3431838
          def file_md5(fname):
              hash_md5 = hashlib.md5()
              with open(fname, "rb") as f:
                  for chunk in iter(lambda: f.read(4096), b""):
                      hash_md5.update(chunk)
              return hash_md5.hexdigest()
          
          def file_sha256_checksum_base64(fname):
              hash_sha256 = hashlib.sha256()
              with open(fname, "rb") as f:
                  for chunk in iter(lambda: f.read(4096), b""):
                      hash_sha256.update(chunk)
    
              return hash_sha256.digest().encode('base64').strip()
              
      Runtime: python2.7
      Timeout: 300

  S3UnzipUploaderFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: RoleActions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource:
                  - Fn::Sub: ${S3UnzipUploaderFunctionLogGroup.Arn}
              - Effect: Allow
                Action:
                  - codepipeline:PutJobSuccessResult
                  - codepipeline:PutJobFailureResult
                Resource: '*'
              - Effect: Allow
                Action:
                  - s3:PutObject
                Resource:
                  Fn::Sub: arn:aws:s3:::${ArtifactStoreBucket}/*
  
  S3UnzipUploaderFunctionLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName:
        Fn::Sub: /aws/lambda/${ProjectGlobalPrefix.Prefix}-S3UnzipUploaderFunction
  
  
  
  CloudFormationTemplateS3KeyUpdaterFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName:
        Fn::Sub: ${ProjectGlobalPrefix.Prefix}-CloudFormationTemplateS3KeyUpdaterFunction
      Description: Replaces the S3 keys in a CloudFormation template's resources
      Handler: index.lambda_handler
      MemorySize: 1536
      Role:
        Fn::Sub: ${CloudFormationTemplateS3KeyUpdaterFunctionRole.Arn}
      Code:
        ZipFile: |-
          import os
          import json
          import re
          import boto3
          import botocore
          
          codepipeline_client = boto3.client("codepipeline")
          
          def lambda_handler(event, context):
              print("Event: {}".format(json.dumps(event)))
              
              job_props = event["CodePipeline.job"]
              artifact_creds = job_props["data"]["artifactCredentials"]
              user_params = json.loads(job_props["data"]["actionConfiguration"]["configuration"]["UserParameters"])
              
              artifact_session = boto3.session.Session(
                  aws_access_key_id = artifact_creds["accessKeyId"],
                  aws_secret_access_key = artifact_creds["secretAccessKey"],
                  aws_session_token = artifact_creds["sessionToken"]
              )
              
              artifact_s3_client = artifact_session.client("s3", config=botocore.client.Config(signature_version='s3v4'))
              
              input_artifact_map = {}
              
              for each_artifact_dict in job_props["data"]["inputArtifacts"]:
                  input_artifact_s3_location = each_artifact_dict["location"]["s3Location"]
              
                  target_bucket_name = input_artifact_s3_location["bucketName"]
                  target_object_key = input_artifact_s3_location["objectKey"]
                  
                  response = artifact_s3_client.get_object(
                      Bucket = target_bucket_name,
                      Key = target_object_key
                  )
                  
                  input_artifact_map[each_artifact_dict["name"]] = response["Body"].read()
              
              deployed_artifact_map = json.loads(input_artifact_map["DeployedArtifactMap"])
              template_string = input_artifact_map["SourceCloudFormationStackTemplate"]
              
              modified_template_string = str(template_string)
              
              change_count = 0
              for each_artifact_key in deployed_artifact_map.keys():
                  each_artifact_value = deployed_artifact_map[each_artifact_key]
                  
                  string_before = str(modified_template_string)
                  modified_template_string = re.sub(":\s+" + re.escape(each_artifact_key), ": " + each_artifact_value, modified_template_string)
                  if string_before != modified_template_string:
                      change_count += 1
                  else:
                      print("No substitution made for {}.".format(each_artifact_key))
              
              print("Made changes to source template {} time(s).".format(change_count))
              
              output_artifact_s3_location = job_props["data"]["outputArtifacts"][0]["location"]["s3Location"]
              
              output_artifact_bucket_name = output_artifact_s3_location["bucketName"]
              output_artifact_object_key = output_artifact_s3_location["objectKey"]
              
              print("Uploading to {} {}".format(
                  output_artifact_bucket_name,
                  output_artifact_object_key
              ))
              
              artifact_s3_client.put_object(
                  Bucket = output_artifact_bucket_name,
                  Key = output_artifact_object_key,
                  Body = modified_template_string
              )
              
              codepipeline_client.put_job_success_result(
                  jobId = job_props["id"]
              )
              
              return {}
              
      Runtime: python2.7
      Timeout: 300

  CloudFormationTemplateS3KeyUpdaterFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: RoleActions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource:
                  - Fn::Sub: ${CloudFormationTemplateS3KeyUpdaterFunctionLogGroup.Arn}
              - Effect: Allow
                Action:
                  - codepipeline:PutJobSuccessResult
                  - codepipeline:PutJobFailureResult
                Resource: '*'
  
  CloudFormationTemplateS3KeyUpdaterFunctionLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName:
        Fn::Sub: /aws/lambda/${ProjectGlobalPrefix.Prefix}-CloudFormationTemplateS3KeyUpdaterFunction
  
  
  
  CodePipelineArtifactExtractorFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName:
        Fn::Sub: ${ProjectGlobalPrefix.Prefix}-CodePipelineArtifactExtractorFunction
      Description: Extracts a single file from a CodePipeline artifact
      Handler: index.lambda_handler
      MemorySize: 1536
      Role:
        Fn::Sub: ${CodePipelineArtifactExtractorFunctionRole.Arn}
      Code:
        ZipFile: |-
          import os
          import json
          import tempfile
          import zipfile
          import shutil
          import boto3
          import botocore
          from botocore.client import Config
          
          zip_output = False
          
          codepipeline_client = boto3.client("codepipeline")
          
          def lambda_handler(event, context):
              print("Event: {}".format(json.dumps(event)))
              
              job_props = event["CodePipeline.job"]
              artifact_creds = job_props["data"]["artifactCredentials"]
              user_params = json.loads(job_props["data"]["actionConfiguration"]["configuration"]["UserParameters"])
              
              artifact_session = boto3.session.Session(
                  aws_access_key_id = artifact_creds["accessKeyId"],
                  aws_secret_access_key = artifact_creds["secretAccessKey"],
                  aws_session_token = artifact_creds["sessionToken"]
              )
              
              artifact_s3_client = artifact_session.client("s3", config=Config(signature_version='s3v4'))
              
              f = tempfile.NamedTemporaryFile(delete=False)
              f.close()
              input_temp_file_path = f.name
              f = tempfile.NamedTemporaryFile(delete=False)
              f.close()
              output_temp_file_path = f.name
              f = tempfile.NamedTemporaryFile(delete=False)
              f.close()
              temp_dir_path = f.name
              os.unlink(temp_dir_path)
              os.makedirs(temp_dir_path)
              
              input_artifact_s3_location = job_props["data"]["inputArtifacts"][0]["location"]["s3Location"]
              
              input_artifact_bucket_name = input_artifact_s3_location["bucketName"]
              input_artifact_object_key = input_artifact_s3_location["objectKey"]
              
              print("Downloading input artifact from {} {}".format(
                  input_artifact_bucket_name,
                  input_artifact_object_key
              ))
              
              artifact_s3_client.download_file(
                  input_artifact_bucket_name,
                  input_artifact_object_key,
                  input_temp_file_path
              )
              
              input_zfile = zipfile.ZipFile(input_temp_file_path)
              input_zfile.extractall(temp_dir_path)
              input_zfile.close()
              
              output_artifact_s3_location = job_props["data"]["outputArtifacts"][0]["location"]["s3Location"]
              
              target_input_file_path = os.path.join(temp_dir_path, user_params["FilePath"])
              
              if zip_output:
                  output_zfile = zipfile.ZipFile(output_temp_file_path, "w")
                  output_zfile.write(
                      target_input_file_path,
                      user_params["FilePath"].split("/")[-1]
                  )
                  output_zfile.close()
              else:
                  shutil.copyfile(target_input_file_path, output_temp_file_path)
              
              print("Uploading {} to {} {}".format(
                  user_params["FilePath"],
                  output_artifact_s3_location["bucketName"],
                  output_artifact_s3_location["objectKey"]
              ))
              
              artifact_s3_client.upload_file(
                  output_temp_file_path,
                  output_artifact_s3_location["bucketName"],
                  output_artifact_s3_location["objectKey"]
              )
              
              codepipeline_client.put_job_success_result(
                  jobId = job_props["id"]
              )
              
              for each_path in [input_temp_file_path, output_temp_file_path]:
                  if os.path.exists(each_path):
                      os.unlink(each_path)
              
              if os.path.exists(temp_dir_path):
                  shutil.rmtree(temp_dir_path)
              
              return {}
              
      Runtime: python2.7
      Timeout: 300

  CodePipelineArtifactExtractorFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: RoleActions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource:
                  - Fn::Sub: ${CodePipelineArtifactExtractorFunctionLogGroup.Arn}
              - Effect: Allow
                Action:
                  - codepipeline:PutJobSuccessResult
                  - codepipeline:PutJobFailureResult
                Resource: '*'
  
  CodePipelineArtifactExtractorFunctionLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName:
        Fn::Sub: /aws/lambda/${ProjectGlobalPrefix.Prefix}-CodePipelineArtifactExtractorFunction
  
  
  
  CodeBuildInstanceFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName:
        Fn::Sub: ${ProjectGlobalPrefix.Prefix}-CodeBuildInstanceFunction
      Description: Perform a CodeBuild build as a CloudFormation custom resource
      Handler: index.lambda_handler
      MemorySize: 128
      Role:
        Fn::Sub: ${CodeBuildInstanceFunctionRole.Arn}
      Code:
        ZipFile: |-
          import json
          import uuid
          import boto3
          import botocore
          import cfnresponse
          
          codebuild_client = boto3.client("codebuild")
          sfn_client = boto3.client("stepfunctions")
          
          def lambda_handler(event, context):
              print("Event: {}".format(json.dumps(event)))
              
              if event.get("RequestType") in ["Create", "Update"]:
                  # This is the initial call from CloudFormation.
                  
                  resource_props = event["ResourceProperties"]
                  
                  response = sfn_client.start_execution(
                      stateMachineArn = resource_props["StateMachineArn"],
                      name = str(uuid.uuid4()),
                      input = json.dumps({
                          "cf-event": event,
                          "fatal-error": False,
                          "build-complete": False
                      })
                  )
                  
                  print("Execution ARN: {}".format(response["executionArn"]))
                
              elif event.get("RequestType") in ["Delete"]:
                  # This is also an update or delete call from CloudFormation.
                  
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {}, None)
                  
                  return {}
              
              else:
                  return safe_state_machine_handler(event, context)
              
          
          def safe_state_machine_handler(event, context):
              try:
                  return state_machine_handler(event, context)
              except Exception as e:
                  event["error-message"] = str(e)
                  event["fatal-error"] = True
                  return event
              
          def state_machine_handler(event, context):
              
              if event.get("fatal-error", False):
                  # The state machine encountered a fatal error.
                  cfnresponse.send(event["cf-event"], context, cfnresponse.FAILED, {}, None)
              
                  return {}
          
              elif "cf-event" in event:
                  # This is an invocation from the Step Functions state machine.
              
                  if "build-id" not in event:
                      # The build hasn't been started yet.
                  
                      resource_props = event["cf-event"]["ResourceProperties"]
                      
                      start_build_kwargs = {
                          "projectName": resource_props["CodeBuildProject"]
                      }
                      
                      if len(resource_props.get("SourceVersion", "")):
                          start_build_kwargs["sourceVersion"] = resource_props["SourceVersion"]
                      
                      response = codebuild_client.start_build(**start_build_kwargs)
                  
                      event["build-id"] = response["build"]["id"]
                  
                      return event
              
                  else:
                      # The build has been started. Check on its progress.
                  
                      response = codebuild_client.batch_get_builds(
                          ids = [event["build-id"]]
                      )
                  
                      this_build = response["builds"][0]
                      
                      if this_build["buildComplete"]:
                          if this_build["buildStatus"] == "SUCCEEDED":
                              
                              cfnresponse.send(event["cf-event"], context, cfnresponse.SUCCESS, {}, None)
                              event["build-complete"] = True
                          else:
                              
                              cfnresponse.send(event["cf-event"], context, cfnresponse.FAILED, {}, None)
                              
                              event["fatal-error"] = True
                              event["error-message"] = "Build completed with status: {}".format(this_build["buildStatus"])
                              
                  
                      return event
              
              else:
                  raise Exception("Unexpected input event.")
              
      Runtime: python2.7
      Timeout: 300

  CodeBuildInstanceFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: RoleActions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource:
                  - Fn::Sub: ${CodeBuildInstanceFunctionLogGroup.Arn}
              - Effect: Allow
                Action:
                  - states:StartExecution
                Resource:
                  Fn::Sub: ${CodeBuildInstanceStateMachine.StateMachineArn}
              - Effect: Allow
                Action:
                  - codebuild:StartBuild
                  - codebuild:BatchGetBuilds
                Resource:
                  - Fn::Sub: ${DockerBuilderCodeBuildProject.Arn}
                  - Fn::Sub: ${InitialSourceRepositoryPusherCodeBuildProject.Arn}
  
  CodeBuildInstanceFunctionLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName:
        Fn::Sub: /aws/lambda/${ProjectGlobalPrefix.Prefix}-CodeBuildInstanceFunction
  
  CodeBuildInstanceStateMachineRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - Fn::Sub: states.${AWS::Region}.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: RoleActions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource:
                  Fn::Sub: arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:${ProjectGlobalPrefix.Prefix}-CodeBuildInstanceFunction
  
  CodeBuildInstanceStateMachine:
    Type: Custom::StepFunctionStateMachine
    Properties:
      ServiceToken:
        Fn::Sub: ${StepFunctionStateMachineCustomResourceFunction.Arn}
      Definition:
        Fn::Sub: |-
          {
            "Comment": "A state machine for starting a CodeBuild build and waiting for it to complete.",
            "StartAt": "StartBuild",
            "States": {
              "StartBuild": {
                "Type": "Task",
                "Resource": "arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:${ProjectGlobalPrefix.Prefix}-CodeBuildInstanceFunction",
                "Catch": [
                  {
                    "ErrorEquals": ["States.ALL"],
                    "Next": "FatalErrorFallback"
                  }
                ],
                "Next": "WaitForBuildToRun"
              },
              "WaitForBuildToRun": {
                "Type": "Wait",
                "Seconds": 30,
                "Next": "CheckIfBuildIsComplete"
              },
              "CheckIfBuildIsComplete": {
                "Type": "Task",
                "Resource": "arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:${ProjectGlobalPrefix.Prefix}-CodeBuildInstanceFunction",
                "Catch": [
                  {
                    "ErrorEquals": ["States.ALL"],
                    "Next": "FatalErrorFallback"
                  }
                ],
                "Next": "IsBuildCompleteChoice"
              },
              "IsBuildCompleteChoice": {
                "Type": "Choice",
                "Choices": [
                  {
                    "Variable": "$.build-complete",
                    "BooleanEquals": true,
                    "Next": "BuildIsComplete"
                  },
                  {
                    "Variable": "$.fatal-error",
                    "BooleanEquals": true,
                    "Next": "FatalErrorFallback"
                  }
                ],
                "Default": "WaitForBuildToRun"
              },
              "BuildIsComplete": {
                "Type": "Pass",
                "End": true
              },
              "FatalErrorFallback": {
                "Type": "Pass",
                "Result": true,
                "ResultPath": "$.fatal-error",
                "Next": "FatalErrorComplete"
              },
              "FatalErrorComplete": {
                "Type": "Task",
                "Resource": "arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:${ProjectGlobalPrefix.Prefix}-CodeBuildInstanceFunction",
                "Retry": [
                  {
                    "ErrorEquals": [ "States.ALL" ],
                    "IntervalSeconds": 5,
                    "MaxAttempts": 10
                  }
                ],
                "End": true
              }
            }
          }
      RoleArn:
        Fn::Sub: ${CodeBuildInstanceStateMachineRole.Arn}
  
  
  #
  #   Step Function State Machine
  #
  
  StepFunctionStateMachineCustomResourceFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName:
        Fn::Sub: ${ProjectGlobalPrefix.Prefix}-StepFunctionStateMachineCustomResourceFunction
      Description: CloudFormation custom resource for a Step Functions State Machine.
      Handler: index.lambda_handler
      MemorySize: 1536
      Role:
        Fn::Sub: ${StepFunctionStateMachineCustomResourceFunctionRole.Arn}
      Code:
        ZipFile: |-
          """StepFunctionStateMachineCustomResourceFunction

          AWS CloudFormation Custom Resource for a Step Functions State Machine.

          """

          from __future__ import print_function

          import os
          import json
          import uuid
          import time
          import boto3
          import botocore
          import cfnresponse

          sfn_client = boto3.client('stepfunctions')

          def lambda_handler(event, context):
              print('Event: {}'.format(json.dumps(event)))
    
              request_type = event.get("RequestType")
              resource_props = event["ResourceProperties"]
    
              stack_id = event["StackId"]
    
              physical_resource_id = event.get("PhysicalResourceId")
    
              response_dict = {}
    
              if request_type in ["Update", "Delete"]:
                  state_machine_arn = physical_resource_id
                  
                  if state_machine_arn != "Invalid":
                      sfn_client.delete_state_machine(
                          stateMachineArn = state_machine_arn
                      )
    
              if request_type == "Update":
                  state_machine_arn = physical_resource_id
        
                  print("Waiting for state machine to be deleted.")
        
                  while True:
                      try:
                          response = sfn_client.describe_state_machine(
                              stateMachineArn = state_machine_arn
                          )
                          print(response["status"])
                      except botocore.exceptions.ClientError as e:
                          if e.response['Error']['Code'] == 'StateMachineDoesNotExist':
                              break
                          else:
                              raise
            
                      time.sleep(5)
    
              if request_type in ["Create", "Update"]:
        
                  state_machine_name = get_state_machine_name()
                  
                  try:
                      definition_dict = json.loads(resource_props["Definition"])
                  except ValueError as e:
                      cfnresponse.send(event, context, cfnresponse.FAILED, {}, "Invalid")
                      raise
                  
                  try:
                      response = sfn_client.create_state_machine(
                          name = state_machine_name,
                          definition = json.dumps(definition_dict, indent=4),
                          roleArn = resource_props["RoleArn"]
                      )
                  except botocore.exceptions.ClientError as e:
                      if e.response['Error']['Code'] == 'InvalidDefinition':
                          cfnresponse.send(event, context, cfnresponse.FAILED, {}, "Invalid")
                      raise
        
                  state_machine_arn = response["stateMachineArn"]
        
                  physical_resource_id = state_machine_arn
        
                  response_dict["StateMachineArn"] = state_machine_arn
                  response_dict["StateMachineName"] = state_machine_name
        
        

              cfnresponse.send(event, context, cfnresponse.SUCCESS, response_dict, physical_resource_id)

              return {}

          def get_state_machine_name():
              return "{}-{}".format(
                  os.environ["PROJECT_GLOBAL_PREFIX"],
                  "{}".format(
                      uuid.uuid4()
                  ).replace("-", "")
              )
      Environment:
        Variables:
          PROJECT_GLOBAL_PREFIX:
            Fn::Sub: ${ProjectGlobalPrefix.Prefix}
      Runtime: python2.7
      Timeout: 300
  
  StepFunctionStateMachineCustomResourceFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: RoleActions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource:
                  Fn::Sub: ${StepFunctionStateMachineCustomResourceFunctionLogGroup.Arn}
              - Effect: Allow
                Action:
                  - states:CreateStateMachine
                Resource:
                  Fn::Sub: arn:aws:states:${AWS::Region}:${AWS::AccountId}:*
              - Effect: Allow
                Action:
                  - states:DeleteStateMachine
                  - states:DescribeStateMachine
                  - states:ListExecutions
                Resource:
                  Fn::Sub: arn:aws:states:${AWS::Region}:${AWS::AccountId}:stateMachine:${ProjectGlobalPrefix.Prefix}-*
              - Effect: Allow
                Action:
                  - states:StopExecution
                Resource:
                  Fn::Sub: arn:aws:states:${AWS::Region}:${AWS::AccountId}:execution:${ProjectGlobalPrefix.Prefix}-*
              - Effect: Allow
                Action:
                  - iam:PassRole
                Resource:
                  Fn::Sub: ${CodeBuildInstanceStateMachineRole.Arn}
  
  StepFunctionStateMachineCustomResourceFunctionLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName:
        Fn::Sub: /aws/lambda/${ProjectGlobalPrefix.Prefix}-StepFunctionStateMachineCustomResourceFunction
  
  #
  #   Docker Image Repository Cleanup
  #   
  #   An ECR repository can't be deleted unless it contains no images. This 
  #   deletes those images so the stack can be deleted.
  #
  
  DockerImageRepositoryCleanupFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName:
        Fn::Sub: ${ProjectGlobalPrefix.Prefix}-DockerImageRepositoryCleanupFunction
      Description: Clears an ECR repository upon stack deletion
      Handler: index.lambda_handler
      MemorySize: 128
      Role:
        Fn::Sub: ${DockerImageRepositoryCleanupFunctionRole.Arn}
      Code:
        ZipFile: |-
          import json
          import boto3
          import botocore
          import cfnresponse
          
          ecr_client = boto3.client("ecr")
          
          def lambda_handler(event, context):
              print("Event: {}".format(json.dumps(event)))
              
              resource_props = event["ResourceProperties"]
              
              if event["RequestType"] == "Delete":
                  
                  repository_name = resource_props["EcrRepositoryName"]
                  
                  response_iterator = ecr_client.get_paginator("list_images").paginate(
                      repositoryName = repository_name
                  )
                  
                  try:
                      for each_response in response_iterator:
                          image_id_list = []
                      
                          for each_image_dict in each_response["imageIds"]:
                              image_id_list.append({
                                  "imageDigest": each_image_dict["imageDigest"]
                              })
                      
                          if len(image_id_list) == 0:
                              continue
                      
                          delete_response = ecr_client.batch_delete_image(
                              repositoryName = repository_name,
                              imageIds = image_id_list
                          )
                      
                          if len(delete_response.get("failures", [])) > 0:
                              raise Exception("Failure occurred in deleting one or more ECR images.")
                              
                  except botocore.exceptions.ClientError as e:
                      if e.response['Error']['Code'] == 'AccessDeniedException':
                          print("Access denied. Assuming no images need deleting.")
                          pass
                      else:
                          raise
                  
              
              cfnresponse.send(event, context, cfnresponse.SUCCESS, {}, None)
              
      Runtime: python2.7
      Timeout: 300

  DockerImageRepositoryCleanupFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: RoleActions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource:
                  - Fn::Sub: ${DockerImageRepositoryCleanupFunctionLogGroup.Arn}
              - Effect: Allow
                Action:
                  - ecr:ListImages
                  - ecr:BatchDeleteImage
                Resource:
                  Fn::Sub: arn:aws:ecr:${AWS::Region}:${AWS::AccountId}:repository/${DockerBuilderImageRepository}
  
  DockerImageRepositoryCleanupFunctionLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName:
        Fn::Sub: /aws/lambda/${ProjectGlobalPrefix.Prefix}-DockerImageRepositoryCleanupFunction
  
  DockerImageRepositoryCleanupInvocation:
    Type: Custom::DockerImageRepositoryCleanup
    Properties:
      ServiceToken:
        Fn::Sub: ${DockerImageRepositoryCleanupFunction.Arn}
      EcrRepositoryName:
        Ref: DockerBuilderImageRepository
  
  
  #
  #   Stack Cleanup
  #   
  #   This function performs basic setup and cleanup functionality when the 
  #   stack is deleted.
  #   
  #   On delete:
  #     - Clears out artifact S3 bucket
  #
  
  StackCleanupFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName:
        Fn::Sub: ${ProjectGlobalPrefix.Prefix}-StackCleanupFunction
      Description: Clears out the stack's S3 bucket
      Handler: index.lambda_handler
      MemorySize: 1536
      Role:
        Fn::GetAtt:
        - StackCleanupFunctionRole
        - Arn
      Code:
        ZipFile: |-
          from __future__ import print_function

          import json
          import boto3
          import botocore
          import cfnresponse
          
          s3_client = boto3.client("s3")
          
          class LambdaHandler(object):
              
              def __init__(self, context):
                  pass
  
              def handle_event(self, event, context):
                  print("Event: {}".format(json.dumps(event)))
        
                  request_type = event.get("RequestType")
        
                  if request_type == "Delete":
                      self.handle_cleanup_event(event, context)
        
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {}, None)
        
                  return {}
    
              def handle_cleanup_event(self, event, context):
        
                  s3_bucket_name = event["ResourceProperties"]["Bucket"]
        
                  paginator = s3_client.get_paginator("list_objects_v2")
        
                  response_iterator = paginator.paginate(
                      Bucket = s3_bucket_name
                  )
        
                  for each_list_response in response_iterator:
                    keys_to_delete = []
          
                    for each_item in each_list_response.get("Contents", []):
                        keys_to_delete.append(each_item["Key"])
          
                    if len(keys_to_delete) == 0:
                        print("Last request for objects in {} returned none.".format(
                            s3_bucket_name
                        ))
                        break
                    
                    print("Deleting {} object(s) from {}.".format(
                        len(keys_to_delete),
                        s3_bucket_name
                    ))
          
                    s3_client.delete_objects(
                        Bucket = s3_bucket_name,
                        Delete = {
                            "Objects": list({"Key": x} for x in keys_to_delete)
                        }
                    )
          
                    print("Object(s) deleted.")

          handler_object = None
          def lambda_handler(event, context):
              global handler_object
    
              if handler_object is None:
                  handler_object = LambdaHandler(context)
    
              return handler_object.handle_event(event, context)
              
      Runtime: python2.7
      Timeout: 300
  
  StackCleanupFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: "/"
      Policies:
        - PolicyName: RoleActions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource:
                  - Fn::Sub: ${StackCleanupFunctionLogGroup.Arn}
              - Effect: Allow
                Action:
                  - s3:DeleteObject
                Resource:
                  Fn::Sub: arn:aws:s3:::${ArtifactStoreBucket}/*
              - Effect: Allow
                Action:
                  - s3:ListBucket
                Resource:
                  Fn::Sub: arn:aws:s3:::${ArtifactStoreBucket}
  
  StackCleanupFunctionLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName:
        Fn::Sub: /aws/lambda/${ProjectGlobalPrefix.Prefix}-StackCleanupFunction
  
  StackCleanupInvocation:
    Type: Custom::StackCleanupInvocation
    Properties:
      ServiceToken:
        Fn::Sub: ${StackCleanupFunction.Arn}
      Bucket:
        Ref: ArtifactStoreBucket
  
  
  
  #
  #   Stack Global Prefix Generator
  #   
  #   For proper IAM restrictions, it helps to have a unique prefix. For 
  #   example:
  #     arn:sqs:us-east-1:000011112222:MyUniquePrefix-*
  #   
  #   Normally I'd use the stack name, but it can be up to 128 characters. 
  #   That means I risk exceeding the 80 character limits for an SQS queue 
  #   name (for example) if I just use that.
  #   
  #   This function just generates a unique global prefix that can be used 
  #   by all resources to indicate "this resource belongs to this stack / 
  #   project deployment".
  #
  
  ProjectGlobalPrefixGeneratorFunction:
    Type: AWS::Lambda::Function
    Properties:
      Description: Generates a short sufficiently-random string for stack resources.
      Handler: index.lambda_handler
      MemorySize: 128
      Role:
        Fn::Sub: ${ProjectGlobalPrefixGeneratorFunctionRole.Arn}
      Code:
        ZipFile: |-
          import random
          import string
          import cfnresponse
          
          # The zbase32 alphabet.
          prefix_characters = "ybndrfg8ejkmcpqxot1uwisza345h769"
          
          def lambda_handler(event, context):
              response_dict = {
                  "Prefix": random.choice(string.ascii_lowercase) + ''.join(random.choice(prefix_characters) for _ in range(5))
              }
              cfnresponse.send(event, context, cfnresponse.SUCCESS, response_dict, None)
      Runtime: python2.7
      Timeout: 300

  ProjectGlobalPrefixGeneratorFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Path: "/"
  
  ProjectGlobalPrefix:
    Type: Custom::ProjectGlobalPrefixGeneration
    Properties:
      ServiceToken:
        Fn::Sub: ${ProjectGlobalPrefixGeneratorFunction.Arn}

Outputs:
  ArtifactStoreBucket:
    Value:
      Ref: ArtifactStoreBucket
  CodePipelineViewUrl:
    Value:
      Fn::Sub: https://console.aws.amazon.com/codepipeline/home?region=${AWS::Region}#/view/${BuildPipeline}
  RepositoryCloneUrlHttp:
    Value:
      Fn::GetAtt:
        - MainRepository
        - CloneUrlHttp
  RepositoryCloneUrlSsh:
    Value:
      Fn::GetAtt:
        - MainRepository
        - CloneUrlSsh